\documentclass[letterpaper,twocolumn,11pt,tight]{article}
%\usepackage{usenix}
\usepackage{usenix_orig}

\usepackage{verbatim,alltt,amsmath}
\usepackage{graphics,graphicx,color}
\usepackage{algorithmic,algorithm}
\usepackage{listings,color,xcolor}
\usepackage{times}
%\usepackage[compact]{titlesec} % tighten up whitespace
\usepackage{amsthm}
\usepackage{caption,subcaption}
\usepackage{listings}
\lstdefinestyle{customc}{
    belowcaptionskip=1\baselineskip,
    breaklines=true,
    %frame=L,
    xleftmargin=\parindent,
    language=C,
    showstringspaces=false,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\bfseries\color{green!40!black},
    commentstyle=\itshape\color{purple!40!black},
    identifierstyle=\color{blue},
    stringstyle=\color{orange},
    morekeywords={string,uint64_t,std}
}
\lstset{escapechar=@,style=customc}

\begin{document}
\newtheorem{claim}{Claim}
\newtheorem{defn}{Definition}
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\srcref}[1]{Figure~\ref{fig:#1}}
\newcommand{\clmref}[1]{Claim~\ref{claim:#1}}
% insertfig is a custom command to avoid repetitive code for including figures
% it takes 4 arguments:
% 1. figure width
% 2. file name of the figure
% 3. caption
% 4. label -- note that actual label will be fig:<arg4>
\newcommand{\insertfig}[4]{\begin{figure}\centering\includegraphics[width=#1\linewidth]{#2}\caption{#3}\label{fig:#4}\end{figure}}
% args: same as insertfig, sans width
\newcommand{\insertsrc}[3]{\begin{figure}\centering\lstinputlisting[frame=single]{#1}\caption{#2}\label{fig:#3}\end{figure}}
%\newcommand{\insertsrc}[3]{\begin{figure}\centering\hline\lstinputlisting{#1}\hline\caption{#2}\label{fig:#3}\end{figure}}

\title{Consistent Caching in a Distributed Graph Store}

\author{\rm Gregory D. Hill \qquad
        \\Cornell University}

\maketitle
\begin{abstract}
Much of todays `Big Data' is graph structured, and multiple companies and universities have introduced graph storage systems optimized to store and process this information.
%Graph storage systems Ojk
%Graphs have become ubiquitous in the Information Age.
%With them, graph storage systems optimized for their highly inter-connected structure.
Weaver is a distributed graph store being developed at Cornell that guarantees strong consistency while allowing read queries and write transactions to execute in parallel.
Caching complicated graph query traversal results has the potential to give a significant performance increase,
but caching in a way that is flexible yet maintains consistency prevents a basic cache implementation.

%Traditional software caching mechianisms would not work well in a graph that changes It supports an evolving graph
%It is built for a dynamically changing graph, where user specified queries can run on a consistent snapshot of the graph while it is updating.
In this paper, we propose a system and interface for caching graph query results in software where cached values can be re-interpreted with the context of relevant changes that have happened to the graph between requests.
This forces query creators to check if a cached value is still valid, but allows them to implement more complex functionality such as incrementally re-computing a result from this supplied context.
Implemented in weaver, we our system halved the time to complete certain workloads on a real world graph. % XXX check
\end{abstract}

\section{Introduction}\label{sec:intro}
Graphs have become ubiquitous in the Information Age. 
`Big data' often comes in the form of online social and web networks, but can
also be graph-structured in more diverse settings like biological protein
interaction networks and electric power grids.
Analyzing the structure of how individual data points connect with each other can lead to novel insights and a much better understanding of a dataset as a whole.

Storing and interpreting a graph in a traditional relational or non-relational database can be very inefficient because of the higher degree of interdependencies within the data. Thus, a new class of systems built specifically for storing and querying graphs have been introduced within the past decade~\cite{giraph,tao,powergraph,pregel,memcache_fb,gps,trinity}.

Weaver, a graph store being developed at Cornell, provides ACID transactions for a fully distributed, sharded graphstore, with high throughput and low latency.
It utilizes fully-distributed coordinator that permits unrelated transactions to proceed in parallel while enforcing strict ordering on transactions whose data sets overlap.

Though our system has high throughput potential for small operations, graph queries can often be incredibly data-intensive and high latency.
This is because they often involve visiting a large number of nodes such as in queries like breadth first search, shortest path, and page rank.

With a static graph, these results can be trivially be cached on either the server or client side. However, a small modification anywhere in the graph has the potential to invalidate this result. Because of this, other systems built for changing graphs have looked for performance improvements by caching parts of the graph on clients~\cite{titan}, but these approaches falter when strong consistency of the graph is desired.

%Systems for graphs generally are either graph processing systems that are distributed and highly parallel but lack support for dynamically changing graphs or graph databases which have only weak consistency guarantees. Weaver supports strong consistency for dynamically changing graphs, thus, we wanted to build a way to cache common queries to the graph in a way that was also consistent.

%    With a static graph, caching can be done on the client side. % titan does this
%If they want the result of an identical calculation twice there is no worry for correctness or fancy caching techniques.
%Even for dynamic graphs, no other graph database does caching of requests and usually focuses on caching parts of the graph.
Though we wanted to keep a single, consistent view of the graph, we still saw room for caching to improve performance for queries to a dynamic graph. Usually, changes that are made between requests do not invalidate old cache values, so being able to return without re-running the full computation could greatly improve performance.
In addition, even if a cached query result is incorrect, it is likely most of the graph structure is the same between the requests, and we saw the potential to save parts of old computations in some query types.

In the following text we propose and evaluate a system to provide functionality for caching between requests to a dynamically changing graph in a way that maintains correctness and provides the opportunity for optimizations even when the cached result is no longer valid.
A simple graph query example, ``is node $v$ reachable from node $u$", would benefit greatly from caching.
Without caching, this request requires a breadth or depth first search of the graph.
If we cached the path at the source for a destination node that was reachable, checking if this path still exists for a later query could potentially avoid this large search and save a significant amount of time (while maintaining correctness because we re-check the path).
Maintaining consistency of this result proves difficult, as simply invalidating on a change $u$ or $v$ is not sufficient because the path could be broken at any point.
For a negative reachability caching becomes almost impossible in a large graph: any node that is reachable from $u$ adding an edge could potentially connect the two nodes.
Weaver has a programming framework for developers to define their own queries on the graph, and we wanted to expose a flexible caching API that was sufficiently generic to perform well even for use cases yet to be developed.

In our caching system, query can store a cached value at a node (or multiple nodes) before it returns to the client. However, with this cached value, a custom user defined class, the query must also specify a \emph{watch set} of nodes for that cached value.
When a cached value is retrieved, it is accompanied by a list for every node in the accompanying \emph{watch set} of the changes made at that node between the time of this request and the time the cached value was calculated.
This moves the burden of invalidating the cache to the query program at the time the cached value is retrieved, a practice we call \emph{lazy cache invalidation}.
This implementation also allows the a query to re-compute a result from the cached value and the changes made to the \emph{watch set}, potentially skipping large portions of the computation.

\section{Related work}\label{sec:related}
Graph stores that store static graphs or that are eventually consistent can more effectively cache using more basic techniques.
%/Our system aims to provide stronger guarantees than other graph stores 
% doesn't make sense in database world, so they dont have it
%To our knowledge, caching of this type has not been formally implemented in any other graph stores mainly because it is not applicable to most of the existing prominent systems. 
Static graph systems such as Pregel~\cite{pregel}, Giraph~\cite{giraph}, GraphLab~\cite{powergraph}, and Trinity~\cite{trinity} do not cache queries and instead focus on delivering high performance by processing the graph using more restrictive and parallel query styles. % For a static graph, caching the way we describe it would not be relevant.
Other systems are not distributed (Neo4j~\cite{neo4j} and GraphChi~\cite{graphchi}), or do heavy client side caching of the graph itself (HypergraphDB~\cite{hypergraphdb}, Titan\cite{titan,titan_slides}).
For both of these types, more performance will come from doing more caching for the graph itself and trying to limit disk accesses or network transfers.
Other systems built to handle dynamic graphs, such as systems at Facebook~\cite{tao,memcache_fb}, Cops~\cite{cops}, Eiger~\cite{eiger}, and Trinity~\cite{trinity} have weaker consistency models than Weaver with the hopes of providing greater performance and availability.
In these cases, a more simple timer invalidated cache would likely be faster then our heavyweight, consistent approach.

\section{Weaver Design}\label{sec:wdesign}
In order to give context to the caching system we implemented, we dedicate a section to give further explanation to the architechture of Weaver and it's query framework.
\footnote{Some text and figures in \secref{wdesign} have been taken from \emph{Weaver: A High-Performance, Linearizable, Distributed Graph Store} -- an unpublished submission to NSDI `14 by Ayush Dubey, Robert Escriva, Gregory D. Hill, and Emin G\"un Sirer}


Weaver partitions the graph data across a set of 
    {\em shard servers} and employs a distributed {\em timestamp coordinator} to
    organize and coordinate their activities.
Shard servers house different partitions of the graph, which are stored
    natively as nodes and their associated outgoing edges. Each node is 
    assigned to a designated shard server, known as its {\em home shard}, 
    and optionally replicated to a set of replicas.
Clients send all operations to one of the timestamp servers, which tag each request with ordering-related
    metadata and forward on to the shard-servers that hold the corresponding
    nodes and edges of the graph. 
An auxiliary key-value store~\cite{warp} stores the mapping from nodes to home shards,
    timestamp servers query and update the mapping as they server client
    transactions.
\figref{arch} illustrates the general system architecture and organization.

\insertfig{0.98}{fig/weaver_arch}{Weaver system architecture}{arch}

\subsection{Data and Query Model}

Weaver's architecture supports a general directed graph, consisting of a set of
    nodes (vertices) with directed edges between them.
Node and edges may be labelled with arbitrary named properties, effectively
    creating multiple logical graphs.
For example, an edge $(u,v)$ may have both ``weight=3.0'' and ``color=red'' properties,
    while another edge $(v,w)$ may have just the ``color=blue'' property.
Queries may take the properties of edges and vertices into account when
    performing traversal. For instance, a query may seek to discover all 
neighbors of node $u$ that are reachable through red edges.

Weaver supports a general purpose graph query interface, which enables developers to flexibly traverse large scale graphs.
Queries in weaver are defined as {\em node programs}, code snippets written by Weaver users.
Before the addition of caching, node programs implemented the interface shown in \srcref{reach_prog}.

A node program may read all information associated with the node, such as its properties
    and its list of outgoing edges, via the node object which is passed
    as an argument. For example, in a BFS traversal program, the node program would traverse the
    node's edges by enumerating them through its node argument.
A node program may also create and modify per-request state in each node. For instance,
    BFS traversal may store the number of pending replies from a node's neighbors within the
    node itself. It is the programmer's responsibility to manage this state correctly to achieve
    the node program's objectives. This flexibility enables developers to write node programs that 
    implement a wide array of graph algorithms.
Node programs execute in a distributed fashion on the home shards of the nodes on which
    they operate. A node program can start at any set of nodes and spawn child node programs
    on any other set of nodes, typically neighbors of the starting set, through the node 
    program API. 
Weaver ensures that the per-request state is eventually
    garbage collected after the node program has returned to the client.
The overall structure of this API is overall very similar to, and is inspired
by, the established graph processing systems like GraphLab~\cite{powergraph} and
Pregel~\cite{pregel}.
Our contributions for a vertex-centric graph query API are the ability to perform these requests on an evolving graph and the ability to utilize request duration state.

\insertsrc{src/write_api.h}{Update operations in Weaver}{write_api}
\insertsrc{src/reach_program.h}{Node program program interface in Weaver before caching}{reach_prog}

Weaver guarantees that every node program sees a consistent snapshot of the graph, even as it traverses an indeterminate number of nodes and edges across multiple shard servers.
How we maintain this ACID consistency is described in the remainder of this section.

\subsection{Graph Consistency}
In a na\"ive design, ACID transactions could be supported easily by a single server that 
    assigns a unique ID drawn from a strictly increasing counter to each
    transaction. Each graph node and edge would be keep track of a creation time
    copied from the ID of the transaction that created that graph element. Deleted
    nodes and edges would turn into tombstones marked with the deletion time.
    Shard servers executing a transaction $A$ with timestamp $\tau_A$ would 
    make node and edge data available only if the $\tau_A$ was between the creation
    and deletion times of that graph element. 
Such a hypothetical scheme would totally order all transactions and provide a snapshot, 
    but would come with a high cost: the global system throughput would be bottlenecked by the
    throughput of a single server.

Weaver uses a scalable ordering protocol to consistently order transactions
    as they are applied at different shard servers.
The ordering protocol uses double vector
    clocks, which are assigned to each request by the timestamp server that
    receives the request.

The first vector clock, which we refer to as \emph{vector timestamp}, reflects
    a count of requests received by each timestamper.
The timestampers share their local counters with each other using a gossip
    protocol, and thus each timestamper has a local copy of the vector clock.
Upon receiving a transaction from a client, the vector timestamper
    increments its local counter and assigns the entire vector clock to the
    transaction, thereby establishing a \emph{partial order} across timestamps
    generated by different vector timestamper servers.
This operation is entirely local and requires no communication on the critical path 
    to generate the vector clock.

The second vector clock, which we call the \emph{queue timestamp}, makes it
    possible for the shards to know \emph{when} to apply a particular write
    transaction.
Consider a simple system with one timestamper and two shard servers, $A$ and
$B$.  If the timestamper were to tag operation an operation with vector
timestamp 1 and forward it to $A$ and subsequently tag operation an operation
with vector timestamp 2 and forward it to $B$, then $B$ would lack sufficient
information to decide when to apply the operation 2.  Under this hypothetical
scenario, $B$ would be unable to discern the case where operation 2 was its
first operation from the case where operation was destined for $B$ as well, but
was delayed in transit.
To this end, the queue timestamp indicates the number of transactions that
    a timestamper has forwarded to each shard server.
Thus, each shard server maintains a priority queue for each timestamper,
    applying operations in sequential order according to the queue timestamp.

The final component of our coordinator is an event ordering service (EOS) that
    resolves conflicting vector timestamps when they are ambiguous.
The event ordering service is necessary to obtain a precise total order because
    vector timestamps establish only a partial order between different
    operations.
It achieves this by maintaining a directed-acyclic-graph (DAG),
wherein each vertex represents a vector clock, and each edge represents the
happens-before relationship between the two vector clocks.

Most importantly, the event ordering service need only be queried when
    the standard vector clock comparison method does not yield a conclusive
    answer for the order of transactions at the head of each priority queue.
Because the timestampers continually gossip amongst themselves to share
    their vector timestamps, the EOS is only queried for a small percentage of
    the total operations, and does not represent a scalability bottleneck.

Transactions may abort in the case that two concurrent transactions attempt
    to delete the same node or edge.
The transactions which triggers an update to the node map first would succeed,
    and all subsequent transactions which attempt to delete the same graph
    object would be aborted by the timestamper.

For read-only node programs, the shard servers effectively take a snapshot
    of the graph on which the program will operate.
Shard servers efficiently maintain multiple snapshots by embedding timestamp
    information for each node and edge within the graph structure itself,
    retaining old versions until they can be garbage collected.
Each snapshot is maintained for the entire duration of the query, so that the
    traversal may freely cross shard server boundaries while operating on
    a consistent snapshot of the graph.

Dubey et al described the subtle aspects of our ordering mechanism in a submission to NSDI earlier this year. In that paper we also proved that our mechanism ensures a linearizable order of transactions.

\section{Cache Design}\label{sec:cdesign}
Weaver's query cache is implemented as an extension to the node program interface. Behind the API, it uses the timestamps associated with the node programs that read and write the cache to give the node-program creator the information they need to ensure consistency of their query.

We wanted our system to abstract away as much of the internals of our system as possible while being complex enough that a cache value is only invalidated when it needs to be.
Our interface lets the people who write queries for the graph define the rules for how a their cached query results should be invalidated.
On top of this, we don't know how often a cached value will be used and we want to avoid adding checks to operations that update the graph, so we never proactively invalidate a cached value.

%The basic model of how graphs are stored in Weaver motivates the design of our traversal caching system. Weaver stores a directed graph in memory. Nodes are a class that contains edges and possibly other user supplied key value pairs (such as ``name", ``Greg"). With every node, edge, and key/value pair is a creation and deletion time-stamp.
%As the graph is stored across multiple servers, edges specify the id of the machine that neighbor node is stored on as well as the memory location of the node on that machine.
%Read and write operations follow timestamps to give a linearizable system. A node/edge creation operation creates an object with a corresponding create time-stamp, and a deletion operation does not delete the object itself but instead just sets the delete time-stamp for that component. A read query will be given a time-stamp as well, and it examines these create and delete timestamps of the components it reads, only exposing those nodes and edges that were created after and not deleted before the time-stamp of the request. 'Deleted components' can be garbage collected once it is sure all subsequent reads in the system will occur chronologically after their delete timestamps.

%\subsection{Query model}
%Weaver supports a general purpose graph query interface that enables developers to implement their own traversals on large scale graphs.
%Users define \emph{node programs}: code snippets that execute 'from the point of view of a vertex.' A node program has access only to the state it is given and the state stored on that node and with that information decides which nodes to send to next and what to send them. Programming from this perspective allows a high level of parallelism in requests to a system, as many of these snippets can run concurrently on different nodes. Node programs formally implement the interface shown in \srcref{cache_reach_prog}.
%A node program query is assigned a time-stamp before starting, and this time-stamp defines the consistent snapshot of the graph the query will see.
%A node program may read all information associated with the node within that snapshot, such as its properties
%and its list of outgoing edges, via the node object which is passed
%as an argument. For example, in a BFS traversal program, the node program would traverse the
%node's edges by enumerating them through its node argument.
\insertsrc{src/reach_program_caching.h}{Node program program interface in Weaver with caching}{cache_reach_prog}
Instead, we give a node program the information it would need to invalidate a cached value when we retrieve it.
What is relevant to invalidating a cached value is what has changed in the graph since the cached value was stored.
However, not all changes to a graph are important; presumably only changes to a subset of the graph can cause invalidation.
In addition, cache invalidations for graphs may not be all-or-nothing.
It is possible that the result of a user query is only slightly affected by a few changes to the graph and could be calculated with less work than starting over by using this caching context.

\subsection{Caching interface extension}
%Many user defined node-programs can involve large portions of the graph, thus caching could greatly impact the runtime of workloads that run queries multiple times while changing the graph.
%If the graph does change, however, it is easy for cached values to become invalid. Graph structured queries differ in the rules for data invalidation. For example, when caching that one node is reachable from another, adding a node or edge to the graph can't change a positive result. On the other hand, when caching a shortest path calculation between two nodes, adding a node or edge could create a new, shorter path. With the flexibility of the node programming interface, there wouldn't be a way to automatically invalidate cached values based on modifications to the graph that wasn't overly pessimistic in it's cache invalidation.

\insertsrc{src/cache_response.h}{Proposed caching interface}{caching}
The caching interface we chose to achieve these goals is shown in \srcref{caching}. To facilitate user-defined invalidation, we have them record a \emph{watch set} of nodes with every cached value. These nodes define the scope of the context that future lookups will be able to examine to determine if a cached value is still valid. 
In addition, the user defines keys (possibly one) that will lookup this cache value when storing it, this is useful for cached values that can answer multiple different queries. 
Essentially, cache values are indexed by the node program type, the node, and finally a user defined key. 
\insertsrc{src/cache_response.h}{interfaces to caching classes}{cache_response}

    When cached values are returned they are wrapped in a cache\_response class (see \srcref{cache_response}) which also contains the context to decide if the cached value is still valid. This context consists of the changes to the \emph{watch set} for that cached value that occurred chronologically between when the cached value was calculated and when the request fetching it occurs. If the user program decides, based on the context, that the cached value is no longer valid, they can invalidate it using the corresponding method call to the cache\_response class.
    
    To illustrate how this caching interface could be used, we provide two examples. The second also shows how a query result could be incrementally updated from a previous cache value and its context.

\subsubsection{Reachability}
The request ``is node $v$ reachable from node $u$" can be written as a node program that is essentially a breadth-first search. In the case of a positive result, where node $v$ is reachable from node $u$, a cached value could be recorded at every node along the path from $u$ to $v$. At node $u$, the source, we would cache the path found to the destination $v$ as the cached value and the nodes in that path would be the corresponding \emph{watch set}. The id of the destination node $v$ can be the key for the cache lookup (we could also have every node in the path be seperate a key, as all nodes in the path are reachable from the source). 

    When the same request, ``is node $v$ reachable from node $u$", is performed again later, the cached value may or may not still be correct. At the source node, the reachability node program will call get\_cache(``$v$"). Because there was previously a positive response, this will return a cached value and it's context. Checking the context to see if it is still valid could be done programmatically simply by scanning the change list for each node in the path and ensuring it did not sever it's edge to the next node in the path.
    Compiling all the changes for every node in the watch set could be time consuming, possibly involving network transfers if some of the nodes are stored on another server, but it can still save a significant amount of work. In this example above, we only have to check nodes that were on the path instead of performing another breadth-first search. In addition, the program only has to go over the changes to the \emph{watch set} that occurred after the cached value was computed, this list of changes will likely be only a fraction of the data stored at each node.

\subsubsection{Clustering Coefficient}\label{sec:clustering}
The local clustering coefficient for a node is a measure of how interconnected that node's neighbors are with each other. Formally, it is the number of edges between neighbors that exist divided by the maximum possible number of edges between those neighbors ($k(k-1)$, where $k$ is the number of neighbors).
The maximum possible coefficient of $1$ represents a clique, where all $n$ of the center node's neighbors are connected to every other neighbor.
To perform the calculation originally, the node program starts at the center node and sends a list of its neighbors to every one of its neighbors.
Each neighbor then compares this list to their set of neighbors and returns to the center the cardinality of the intersection of those sets.
The center node sums these values and divides by $n(n-1)$ to get the clustering coefficient. Now, if we wanted to cache this value using our framework, we would record the coefficient as the cache\_value, the watch set as the center node and its neighbors, and the key could as a constant value for all requests (it is not really needed here).

    When this cached result is read later it could be used by itself or updated based on the context, potentially avoiding re-calculating the value completely. Examining the changes to the \emph{watch set}, the cached value is still valid if one of the neighbors added or deleted an edge that was not also a neighbor of the center. If the center added or deleted an edge the cache value would be invalid. In the third case, if a neighbor added or deleted an edge to a node that was also a neighbor of the center, the clustering coefficient is equal to the cached coefficient plus or minus $\frac1{n(n-1)}$. 
We believe this type of incremental updating of a cached value could significantly increase the throughput of graph stores that handle a lot of online queries. In this case, we potentially avoid exchanging neighbor sets and doing set intersections even if the local graph structure has changed in between two requests.

\section{Implementation}\label{sec:design}
\subsection{Cache Backend}
The backend for the cache is implemented a hash table for each node that is keyed by request type and the key of the cached value. A cache lookup is performed by setting a boolean and desired key to lookup on a node in the node program parameters that are passed to it.
When this flag has been set, the shard attempts to fetch the cache value, including context, before starting the node program at that node. When any node program is run, a function to add a cache value is passed in a argument to the node program (in addition to the reference to the node itself, the program state at that node, and the parameters passed by the previous caller as described in \srcref{cache_reach_prog}). This setter is a wrapper for a partially bound function (using std::bind) that adds the value with a relevant timestamp to the cache hash table for that node.We used this bind to provide the most simple interface we could to the node program writer. 
\subsection{Context Fetching}
Every cache value is stored with the vector timestamp for when it was created and the user defined watch set supplied when the value was set. Fetching this context commonly involves reading data that exists on another shard. Thus, all the requisite data for running the node-program that will recieve a cache response with context must be saved on the heap while all the nodes of the watch set are queried to determine their node\_cache\_context.
This data is fetched by sending concurrent batched queries to the shards containing the nodes in the watch set. We find this context fetching to be faster than running a node program itself. To get the context for each node we iterate through it's contents, comparing the timestamps of edges and properties on that node with the timestamp of the cached value and the timestamp of the request fetching it. %These batched queries also have less network overhead than running node programs.
%\subsection{Message Batching}
%To reduce network communication, message batching will be used when fetching the context for a cached value from the nodes in the \emph{watched set}.
%When compiling the context for the watch set we must query each node in the watch set for the changes that have happened in the time between the cached request and the current one. We will send one message to each relevant server with the nodes on that server we would like the changes in this time range for, and it will compile the results and send them back in a batch as well. Weaver message batches already when running node programs, where parallel requests crossing to another machine are grouped together to minimize the overhead of network communication.
%\subsection{Time Range Slicing}
%This caching implementation extensively uses time range queries for node information, and the way we store this data should be re-designed to make these queries efficient. Though we originally stored edges and properties in their own re-sizable arrays, this structure makes figuring out what happened between two timestamps difficult. Instead, we plan to implement the all modifications to a node, including adding or deleting edges or properties or deleting the node itself, to be stored in an append only log for that node. An append-only log is possible because our system ensures strongly consistent transactions and thus we know that all updates to a single node will arrive in chronological order. Finding all changes made after a given timestamp to this log is now a simple binary search on the timestamps of the log. Previous data-structures for only edges or only key/value properties can be replaced by filtering iterators over this log without much overhead. We also hope that having all but the head of the log immutable will make it possible to have reads be lock-free, though extra logic would need to be added to resize and compact these logs as the graph mutates.
\subsection{Cached Node Programs}
We have implemented our query caching in both the reachability and clustering coefficient node programs.

For reachability, we only cache positive results. This is done by backtracing the path found from the source to the destination and adding a cached value for the destination for each node along the path. The watch\_set for these cache values is the sub-path from that location in the graph to the destination. Reachability programs check for cached values at every node they go to. They check the context by making sure the path was not broken. If it was, the cached value is invalidated and the request continues without caching.

For clustering, we cache the numerator of the coefficient calculation at the center node for the request. The watch set is that center node and all its neighbors. The cached value is incrementally updated as described in \secref{clustering} and otherwise invalidated.
\section{Evaluation Plan}\label{sec:design}
A good evaluation of our caching system will mainly consist of comparisons of throughput and latency with and without caching. We hope to have caching examples to benchmark for reachability and clustering coefficient, two node programs already implemented in our system. Unfortunately, there is a plethora of variables to deal with when benchmarking graph databases. We could vary the type of graph, the size of the graph, how the graph is mutating over time, the pattern of requests made, and the number of servers the system is running on. To focus on cache performance, we would like to keep the system fairly small, maybe 4-8 nodes. This number is also close to how many machines on the systems cluster would be available for testing. we would like to work with both real-world (like a Twitter subset) and synthetic graphs (like $G_{n,p}$ random graphs). The sizes of these graphs will probably be chosen based on what is available for the real-world graphs and what can be benchmarked in a reasonable amount of time for generated graphs. One point of difficulty is how to test incremental caching techniques on evolving graphs. We will have to look around for datasets that record graph structure but also include timestamps so the evolution of the graph could be replayed in our system. For synthetic graphs, we will look for models for graph evolution, but they may not exist yet.

\subsection{Other applications}
We would also like to design and implement one or two other applications for node programs with caching in our system. A top contender currently would be a program that would do a basic friend recommendation algorithm which would rank the two-hop neighbors of a given node by the number of neighbors they share with it. In a social network, the top of this list would contain the people with the most mutual friends with the individual we wanted recommendations for (that weren't already friends with that user). we think this application could have a clear use case for a dynamic graph and caching. It makes sense that it would be better to re-compute recommendations every time a user logs in instead of doing it offline every few hours. Doing these computations online make sure recommendations do not lag behind changes to the social network and could allow someone who has just joined the social network to establish themselves in it faster. It is also a use case where results caching seems logical, as often the results will be unchanged or only slightly different from a previous calculation.
\bibliographystyle{plain}
\footnotesize
\bibliography{bib_nsdi}
\end{document}
